{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "df1 = pd.read_csv('D://RPI/Daily_files/Datathon/complaints_data/complaints.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Product']=='Credit reporting','Product'] = 'Credit reporting, credit repair services, or other personal consumer reports'\n",
    "df.loc[df['Product']=='Credit card','Product'] = 'Credit card or prepaid card'\n",
    "df.loc[df['Product']=='Bank account or service','Product'] = 'Checking or savings account'\n",
    "df.loc[df['Product']=='Consumer Loan','Product'] = 'Payday loan, title loan, or personal loan'\n",
    "df.loc[df['Product']=='Payday loan','Product'] = 'Payday loan, title loan, or personal loan'\n",
    "df.loc[df['Product']=='Money transfers','Product'] = 'Money transfer, virtual currency, or money service'\n",
    "df.loc[df['Product']=='Prepaid card','Product'] = 'Credit card or prepaid card'\n",
    "df.loc[df['Product']=='Virtual currency','Product'] = 'Money transfer, virtual currency, or money service'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Product'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = df.loc[df['Product']=='Mortgage',:]\n",
    "#a.drop(columns = ['Consumer complaint narrative','Company public response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('D://RPI/Daily_files/Datathon/complaints_data/complaints2_0.csv', sep = ';')\n",
    "#a.to_csv('D://RPI/Daily_files/Datathon/complaints_data/complaints_mortage.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#date features\n",
    "import time\n",
    "from datetime import datetime\n",
    "df['Date received'] = pd.to_datetime(df['Date received'], format = '%Y-%m-%d')\n",
    "df['Date sent to company'] = pd.to_datetime(df['Date sent to company'], format = '%Y-%m-%d')\n",
    "\n",
    "#only use data after 20170501\n",
    "b = df.loc[df['Date received'] >='2017-05-01',:]\n",
    "\n",
    "#only use top 5 product data\n",
    "b = b[b['Product'].isin(['Credit reporting, credit repair services, or other personal consumer reports','Mortgage','Debt collection','Credit card or prepaid card','Checking or savings account','Company public response'])]\n",
    "c = b.copy()\n",
    "\n",
    "#drop nlp data and features with much null values\n",
    "b = b.drop(columns = ['Consumer complaint narrative','Consumer disputed?','Tags'])\n",
    "#rename the products\n",
    "b.loc[b['Product']=='Credit reporting, credit repair services, or other personal consumer reports','Product'] = 'CreditRpt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = c.loc[c['Product']=='Credit reporting, credit repair services, or other personal consumer reports',:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b['Product'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 706853 entries, 0 to 1548291\n",
      "Data columns (total 15 columns):\n",
      "Date received                   706853 non-null datetime64[ns]\n",
      "Product                         706853 non-null object\n",
      "Sub-product                     706853 non-null object\n",
      "Issue                           706853 non-null object\n",
      "Sub-issue                       626253 non-null object\n",
      "Company public response         371752 non-null object\n",
      "Company                         706853 non-null object\n",
      "State                           689807 non-null object\n",
      "ZIP code                        619969 non-null object\n",
      "Consumer consent provided?      564791 non-null object\n",
      "Submitted via                   706853 non-null object\n",
      "Date sent to company            706853 non-null datetime64[ns]\n",
      "Company response to consumer    706852 non-null object\n",
      "Timely response?                706853 non-null object\n",
      "Complaint ID                    706853 non-null int64\n",
      "dtypes: datetime64[ns](2), int64(1), object(12)\n",
      "memory usage: 86.3+ MB\n"
     ]
    }
   ],
   "source": [
    "#b['Product'].value_counts()\n",
    "#print(b.isna().sum())\n",
    "#print(len(b))\n",
    "b.info()\n",
    "\n",
    "#b['Sub-product'].value_counts()\n",
    "#b['Issue'].value_counts()\n",
    "#b['Sub-issue'].value_counts()\n",
    "#b['Company'].value_counts()\n",
    "#b['Product'].value_counts()\n",
    "#b['Company response to consumer'].value_counts()\n",
    "#b['Timely response?'].value_counts()\n",
    "\n",
    "#b.to_csv('complaints_top5product.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programming apps\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#about nlp data- c\n",
    "#c = c['Consumer complaint narrative']\n",
    "#c.dropna(inplace = True)\n",
    "\n",
    "c.to_csv('D://RPI/Daily_files/Datathon/complaints_data/narrative.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp1\n",
    "#https://blog.csdn.net/HuangZhang_123/article/details/80277793\n",
    "def getText():\n",
    "    txt=open(\"D://RPI/Daily_files/Datathon/complaints_data/narrative.txt\",\"r\",encoding='utf-8').read() #打开文件\n",
    "    txt=txt.lower()                  #将所有单词转换为小写去掉大小写的干扰\n",
    "    for ch in '`!@#~$%^&*()_+-=*/{}[];,./?<>': #去掉所有的特殊符号\n",
    "        txt=txt.replace(ch,\" \")   #将特殊符号替换成空格 即去掉\n",
    "    return txt\n",
    "hmltTxt=getText()    #对文件进行读取\n",
    "words=hmltTxt.split()\n",
    "#因为现在单词间均为空格分隔开来，所以用split用空格分隔他们并变成列表返回\n",
    "counts={} #建立一个字典\n",
    "for word in words:\n",
    "    counts[word]=counts.get(word,0)+1\n",
    "    #用当前的某一个单词作为键索引字典 如果在里面则返回次数再加一 若不在里面则直接加1\n",
    "items=list(counts.items())\n",
    "#用list将counts变为一个列表类型  counts.items()-->返回可遍历的（键，值）元组数组\n",
    "items.sort(key=lambda x:x[1],reverse=True)\n",
    "#使用list.sort()方法来排序，此时list本身将被修改\n",
    "for i in range(100):\n",
    "    word,count=items[i]         \n",
    "    print(\"{0:<10}{1:>5}\".format(word,count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp2 \n",
    "#https://blog.csdn.net/qq_41839921/article/details/83653126\n",
    "import re\n",
    "import functools\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os, sys\n",
    " \n",
    "# 把文本内容整理成一个word list\n",
    "txt=open('D://RPI/Daily_files/Datathon/complaints_data/narrative.txt')\n",
    "readl=txt.readline()\n",
    " \n",
    " \n",
    "r='[’!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^`{|}~]+'\n",
    "word_list2 = []\n",
    "while readl:\n",
    "    readl = txt.readline()\n",
    "    ff = str(readl)\n",
    "    ff=re.sub(r,' ',ff) \n",
    "    words=ff.split(' ') \n",
    "    for word in words:\n",
    "        word_list2.append(word)    \n",
    "#print(word_list2)\n",
    "txt.close()\n",
    " \n",
    " \n",
    "#统计频次\n",
    "tf = {}\n",
    "for word in word_list2:\n",
    "    word = word.lower()\n",
    "        # print(word)\n",
    "    word = ''.join(word.split())\n",
    "    if word in tf:\n",
    "        tf[word] += 1\n",
    "    else:\n",
    "        tf[word] = 1\n",
    "print(tf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp3 https://blog.csdn.net/tanlangqie/article/details/80493888\n",
    "!pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp3 https://blog.csdn.net/tanlangqie/article/details/80493888\n",
    "# - * - coding: utf - 8 -*-\n",
    "#fun: 利用jieba分词和wordcloud创建分词与词用\n",
    "# time：2018-5-29\n",
    "# author： tang\n",
    "# reference： http://blog.csdn.net/fontthrone\n",
    "\n",
    "from os import path\n",
    "from scipy.misc import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import jieba\n",
    "# jieba.load_userdict(\"txt\\userdict.txt\")\n",
    "# 添加用户词库为主词典,原词典变为非主词典\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "\n",
    "# 获取当前文件路径\n",
    "# __file__ 为当前文件, 在ide中运行此行会报错,可改为\n",
    "# d = path.dirname('.')\n",
    "d = path.dirname('D://RPI/Daily_files/Datathon/complaints_data/narrative.txt')\n",
    "\n",
    "stopwords = {}\n",
    "isCN = 0 #默认启用中文分词\n",
    "back_coloring_path = r\"./Image/narrative_pic.jpg\" # 设置背景图片路径\n",
    "text_path = r'D://RPI/Daily_files/Datathon/complaints_data/narrative.txt' #设置要分析的文本路\n",
    "#font_path = r'./fonts\\simkai.ttf' # 为matplotlib设置中文字体路径没\n",
    "stopwords_path = r'D://RPI/Daily_files/Datathon/complaints_data/stopword111.txt' # 停用词词表\n",
    "imgname1 = r\"D://RPI/Daily_files/Datathon/complaints_data/WordCloudDefautColors.png\" # 保存的图片名字1(只按照背景图片形状)\n",
    "imgname2 = r\"D://RPI/Daily_files/Datathon/complaints_data/WordCloudColorsByImg.png\"# 保存的图片名字2(颜色按照背景图片颜色布局生成)\n",
    "\n",
    "#my_words_list = ['一夜爆红'] # 在结巴的词库中添加新词\n",
    "\n",
    "back_coloring = imread(r\"D://RPI/Daily_files/Datathon/complaints_data/bg.jpg\")# 设置背景图片-----back_coloring 为3维数组\n",
    "\n",
    "# 设置词云属性\n",
    "#wc = WordCloud(font_path='黑体.ttf',  # 设置字体\n",
    "#               background_color=\"white\",  # 背景颜色\n",
    "#               max_words=2000,  # 词云显示的最大词数\n",
    "#               mask=back_coloring,  # 设置背景图片\n",
    "#               max_font_size=100,  # 字体最大值\n",
    "#               min_font_size=20,\n",
    " #              random_state=42,\n",
    " #              width=1000, height=860, margin=2,# 设置图片默认的大小,但是如果使用背景图片的话,\n",
    "                                    # 那么保存的图片大小将会按照其大小保存,margin为词语边缘距离\n",
    "#               )\n",
    "\n",
    "# 添加自己的词库分词，例如词库中没有一夜爆红这个词，我们可以添加：一夜爆红\n",
    "#def add_word(list):\n",
    "#    for items in list:\n",
    "#        jieba.add_word(items)\n",
    "\n",
    "#add_word(my_words_list)\n",
    "\n",
    "text = open( text_path, encoding='utf-8').read()      #打开文本，获取内容\n",
    "\n",
    "def jiebaclearText(text):\n",
    "    mywordlist = []                                #存放最终分词结果\n",
    "    seg_list = jieba.cut(text, cut_all=False)\n",
    "    liststr=\"/ \".join(seg_list)                      #未经处理的文本分词结果列表\n",
    "    f_stop = open(stopwords_path, encoding='utf-8')     #打开停用词词表\n",
    "    try:\n",
    "        f_stop_text = f_stop.read( )                      #获取停用词词表中的内容\n",
    "    finally:\n",
    "        f_stop.close( )\n",
    "    f_stop_seg_list = f_stop_text.split('\\n')\n",
    "    for myword in liststr.split('/'):      #获取初次分词结果中的每一个词\n",
    "        if not(myword.strip() in f_stop_seg_list) and len(myword.strip())>1:\n",
    "            mywordlist.append(myword)\n",
    "    return ''.join(mywordlist)\n",
    "\n",
    "if isCN:   #开启中文分词\n",
    "    text = jiebaclearText(text)         #获得中文分词结果\n",
    "\n",
    "# 生成词云, 可以用generate输入全部文本(wordcloud对中文分词支持不好,建议启用中文分词),\n",
    "# 也可以我们计算好词频后使用generate_from_frequencies函数\n",
    "wc.generate(text)\n",
    "wc.generate_from_frequencies(txt_freq)\n",
    "# txt_freq例子为[('词a', 100),('词b', 90),('词c', 80)]\n",
    "\n",
    "#image_colors = ImageColorGenerator(back_coloring)       # 从背景图片生成颜色值\n",
    "\n",
    "plt.figure()\n",
    "# 以下代码只显示--------形状与背景图片一致，颜色为默认颜色的词云\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()     # 绘制词云\n",
    "wc.to_file(imgname1)    # 保存图片\n",
    "\n",
    "\n",
    "# 以下代码显示--------形状与背景图片一致，颜色也与背景图颜色一致的词云\n",
    "image_colors = ImageColorGenerator(back_coloring)        # 从背景图片生成颜色值\n",
    "plt.imshow(wc.recolor(color_func=image_colors))\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "wc.to_file( imgname2)\n",
    "\n",
    "\n",
    "# 绘制以背景图片为颜色的图片----类似于绘制背景图片\n",
    "# plt.figure()\n",
    "# plt.imshow(back_coloring, cmap=plt.cm.gray)\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()\n",
    "# 保存图片\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in d:\\programming apps\\anaconda\\lib\\site-packages (3.4.4)\n",
      "Requirement already satisfied: six in d:\\programming apps\\anaconda\\lib\\site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: pillow in d:\\programming apps\\anaconda\\lib\\site-packages (6.1.0)\n",
      "Requirement already satisfied: lxml in d:\\programming apps\\anaconda\\lib\\site-packages (4.3.4)\n",
      "根据词频，开始生成词云!\n"
     ]
    }
   ],
   "source": [
    "# 词云展示\n",
    "!pip install nltk\n",
    "!pip install pillow\n",
    "!pip install lxml\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from lxml import etree\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "\n",
    "# 去掉停用词\n",
    "def remove_stop_words(f):\n",
    "\n",
    "    f_stop = open('D://RPI/Daily_files/Datathon/complaints_data/stopword111.txt', encoding='utf-8')\n",
    "    try:\n",
    "        f_stop_text = f_stop.read( )                      #获取停用词词表中的内容\n",
    "    finally:\n",
    "        f_stop.close( )\n",
    "    stop_words = f_stop_text.split('\\n')\n",
    "     \n",
    "    for stop_word in stop_words:\n",
    "        f = f.replace(stop_word, '')\n",
    "    return f\n",
    "\n",
    "# 生成词云\n",
    "def create_word_cloud(f):\n",
    "    print('根据词频，开始生成词云!')\n",
    "    f = remove_stop_words(f)\n",
    "    cut_text = word_tokenize(f)\n",
    "    #print(cut_text)\n",
    "    cut_text = \" \".join(cut_text)\n",
    "    wc = WordCloud(\n",
    "        max_words=100,\n",
    "        width=2000,\n",
    "        height=1200,\n",
    "    )\n",
    "    wordcloud = wc.generate(cut_text)\n",
    "\t# 写词云图片\n",
    "    wordcloud.to_file(\"wordcloud.jpg\")\n",
    "    # 显示词云文件\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# 数据加载\n",
    "\n",
    "def getText():\n",
    "    txt=open(\"D://RPI/Daily_files/Datathon/complaints_data/narrative.txt\",\"r\",encoding='utf-8').read() #打开文件\n",
    "    txt=txt.lower()                  #将所有单词转换为小写去掉大小写的干扰\n",
    "    for ch in '`!@#~$%^&*()_+-=*/{}[];,./?<>': #去掉所有的特殊符号\n",
    "        txt=txt.replace(ch,\" \")   #将特殊符号替换成空格 即去掉\n",
    "    return txt\n",
    "hmltTxt=getText()    #对文件进行读取\n",
    "all_word=hmltTxt.split()\n",
    "\n",
    "# 生成词云\n",
    "create_word_cloud(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#只需要product-creditreporting下面的narrative\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from lxml import etree\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "\n",
    "# 去掉停用词\n",
    "def remove_stop_words(f):\n",
    "\n",
    "    f_stop = open('D://RPI/Daily_files/Datathon/complaints_data/stopword111.txt', encoding='utf-8')\n",
    "    try:\n",
    "        f_stop_text = f_stop.read( )                      #获取停用词词表中的内容\n",
    "    finally:\n",
    "        f_stop.close( )\n",
    "    stop_words = f_stop_text.split('\\n')\n",
    "     \n",
    "    for stop_word in stop_words:\n",
    "        f = f.replace(stop_word, '')\n",
    "    return f\n",
    "\n",
    "# 数据加载\n",
    "\n",
    "def getText():\n",
    "    txt=open(\"D://RPI/Daily_files/Datathon/complaints_data/narrative.txt\",\"r\",encoding='utf-8').read() #打开文件\n",
    "    txt=txt.lower()                  #将所有单词转换为小写去掉大小写的干扰\n",
    "    for ch in '`!@#~$%^&*()_+-=*/{}[];,./?<>': #去掉所有的特殊符号\n",
    "        txt=txt.replace(ch,\" \")   #将特殊符号替换成空格 即去掉\n",
    "    return txt\n",
    "hmltTxt=getText()    #对文件进行读取\n",
    "all_word=hmltTxt.split()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
